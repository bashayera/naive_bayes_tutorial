{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Python code for Naive - Bayes Classifier\n",
    "\n",
    "We'll do some cool stuff using what we've learned from the tutorial. You must've seen a spam sms in your phone. Those sms  contains promotions, ads, disguised links, phishing etc. Can we classify these sms from a text from your mom or other important ones using a machine learning classifier? If so, when a new sms comes, the classifier will decide whether it is a spam or ham and don't show notifications. Even we can set up an auto deletion for those sms. Sounds cool? \n",
    "\n",
    "Let's do it using our hero Naive - Bayes Classifier. By the way, Naive - Bayes is well-known for text classification because of the inherent behaviour of that problem. We'll discuss that after sometime. \n",
    "\n",
    "### ML Experiment Pipeline\n",
    "\n",
    "- Get the data\n",
    "- Clean the data\n",
    "- Representation of data\n",
    "- Divide the data into training and testing data\n",
    "- Train the model in training data\n",
    "- Test the model using test data\n",
    "- Find the accuracy of the model in test data\n",
    "\n",
    "## Get the data\n",
    "\n",
    "Get the [SMS Spam Collection Dataset](https://www.kaggle.com/uciml/sms-spam-collection-dataset) from [Kaggle](https://www.kaggle.com/). I've downloaded and extracted it to a .csv format."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "     v1                                                 v2 Unnamed: 2  \\\n",
      "0   ham  Go until jurong point, crazy.. Available only ...        NaN   \n",
      "1   ham                      Ok lar... Joking wif u oni...        NaN   \n",
      "2  spam  Free entry in 2 a wkly comp to win FA Cup fina...        NaN   \n",
      "3   ham  U dun say so early hor... U c already then say...        NaN   \n",
      "4   ham  Nah I don't think he goes to usf, he lives aro...        NaN   \n",
      "\n",
      "  Unnamed: 3 Unnamed: 4  \n",
      "0        NaN        NaN  \n",
      "1        NaN        NaN  \n",
      "2        NaN        NaN  \n",
      "3        NaN        NaN  \n",
      "4        NaN        NaN  \n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "sms_data = pd.read_csv(\"sms_data.csv\", encoding=\"iso-8859-1\")\n",
    "print(sms_data.head())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Clean the data\n",
    "\n",
    "Data won't come to the way we want it to be. So let take the broom and clean it.\n",
    "\n",
    "- Remove the numbers \n",
    "- We'll remove the [stop words in english](http://www.ranks.nl/stopwords) \n",
    "    - [Some extremely common words which would appear to be of little value in helping us to take a decision](https://nlp.stanford.edu/IR-book/html/htmledition/dropping-common-terms-stop-words-1.html)\n",
    "    \n",
    "- The special characters will be removed as well\n",
    "\n",
    "We'll do the first cleaning, th last two will be using `sklearn.feature_extraction.text.CountVectorizer`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "     v1                                                 v2\n",
      "0   ham  Go until jurong point, crazy.. Available only ...\n",
      "1   ham                      Ok lar... Joking wif u oni...\n",
      "2  spam  Free entry in 2 a wkly comp to win FA Cup fina...\n",
      "3   ham  U dun say so early hor... U c already then say...\n",
      "4   ham  Nah I don't think he goes to usf, he lives aro...\n"
     ]
    }
   ],
   "source": [
    "data_cleaned = sms_data.dropna(axis=1, how='any') #drop all the null columns\n",
    "print(data_cleaned.head())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  Label                                               Text\n",
      "0   ham  Go until jurong point, crazy.. Available only ...\n",
      "1   ham                      Ok lar... Joking wif u oni...\n",
      "2  spam  Free entry in 2 a wkly comp to win FA Cup fina...\n",
      "3   ham  U dun say so early hor... U c already then say...\n",
      "4   ham  Nah I don't think he goes to usf, he lives aro...\n"
     ]
    }
   ],
   "source": [
    "data_cleaned.columns = ['Label', 'Text'] #name the columns\n",
    "print(data_cleaned.head())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "message_dict = {} # this will be a dictionary with keys as text and values as labels \n",
    "for i in range(len(data_cleaned)):\n",
    "    message_dict[data_cleaned[\"Text\"][i]] = data_cleaned[\"Label\"][i]\n",
    "    \n",
    "labels = list(message_dict.values())\n",
    "labels = [1 if x == \"ham\" else 2 for x in labels] # converted the labels to numbers i.e. ham is 1, spam is 2\n",
    "messages_temp = list(message_dict.keys())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# removing the digits\n",
    "import re\n",
    "messages = []\n",
    "for msg in messages_temp:\n",
    "    output = re.sub(r'\\d+', ' ', msg)\n",
    "    output = re.sub(r\"\\s+\", \" \", output)\n",
    "    output = output.strip()\n",
    "    messages.append(output)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Cleaned messages\n",
      "=================\n",
      "\n",
      "0. Go until jurong point, crazy.. Available only in bugis n great world la e buffet... Cine there got amore wat...\n",
      "1. Ok lar... Joking wif u oni...\n",
      "2. Free entry in a wkly comp to win FA Cup final tkts st May . Text FA to to receive entry question(std txt rate)T&C's apply over 's\n",
      "3. U dun say so early hor... U c already then say...\n",
      "4. Nah I don't think he goes to usf, he lives around here though\n"
     ]
    }
   ],
   "source": [
    "print(\"Cleaned messages\")\n",
    "print(\"=================\\n\")\n",
    "for i in range(5):\n",
    "    print(str(i) + \". \" + messages[i])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Representation of Data \n",
    "\n",
    "Now ask yourself a question. Can you feed this data directly to classifier? Nope. None of the Machine Learning (ML) algorithms have the ability to use the raw data as inputs. We need to represent our data in a numerical form so that the ML algorithm can take it from there. So how we're going to represent it our sms? There are many representations in NLP to convert a text to a numerical vector. One of them is [Bag of Words](https://en.wikipedia.org/wiki/Bag-of-words_model). \n",
    "\n",
    "In Bag of Words model, we'll find the vocabulary of the text, then represent each word in the vocabulary with its frequency of occuring. \n",
    "\n",
    "For example,\n",
    "\n",
    "Here are two simple text documents:\n",
    "\n",
    "```\n",
    "(1) John likes to watch movies. Mary likes movies too.\n",
    "(2) John also likes to watch football games.\n",
    "```\n",
    "\n",
    "Vocabulary of this text chunk will be:\n",
    "```\n",
    " \"John\"\n",
    " \"likes\"\n",
    " \"to\"\n",
    " \"watch\"\n",
    " \"movies\"\n",
    " \"Mary\"\n",
    " \"too\"\n",
    " \"also\"\n",
    " \"football\"\n",
    " \"games\"\n",
    "```\n",
    "\n",
    "Frequency of vocabulary:\n",
    "\n",
    "```\n",
    "\n",
    "| Word     | Frequency |\n",
    "|----------|-----------|\n",
    "| John     |     2     |\n",
    "| likes    |     3     |\n",
    "| to       |     2     |\n",
    "| watch    |     2     |\n",
    "| movies   |     2     |\n",
    "| Mary     |     1     |\n",
    "| too      |     1     |\n",
    "| also     |     1     |\n",
    "| football |     1     |\n",
    "| games    |     1     |\n",
    "\n",
    "```\n",
    "Now the numerical representation of the a sample sentence will be:\n",
    "\n",
    "```\n",
    "| Sentence                       | John | likes | to | watch | movies | Mary | too | also | football | games |\n",
    "|--------------------------------|------|-------|----|-------|--------|------|-----|------|----------|-------|\n",
    "| Sleeba likes to watch football | 0    |  3    |  2 |  2    | 0      | 0    | 0   | 0    | 1        | 0     |\n",
    "```\n",
    "\n",
    "This is a matrix with dimensions `number of samples x number of features` \n",
    "\n",
    "NB: This one of way of doing it and it's easy. But, let it be in life or numerical representation of text, easy things yield no great results. There are great ways to represent text content like Word2Vec or Glove and you may go through that, but after completing this tutorial. :D\n",
    "\n",
    "#### CountVectorizer\n",
    "\n",
    "[CountVectorizer](http://scikit-learn.org/stable/modules/generated/sklearn.feature_extraction.text.CountVectorizer.html) is a useful function provided by the python machine learning library [scikit-learn](http://scikit-learn.org/stable/index.html). Following processes happen during this step. \n",
    "\n",
    "- Tokenizing the text. — Vectorization and tokenizing\n",
    "- Throw away some less important words. — stop word\n",
    "- Throwing away words that occur way too often to be of any help in detecting relevant posts. — stemming\n",
    "- Throwing away words that occur so seldom that there is only a small chance that they occur in future posts.\n",
    "- Counting the remaining words."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "vectorizer = CountVectorizer(min_df=5, # minimum frequency for a word to be considered to vocabulary\n",
    "                             ngram_range=(1, 1), # for tokenizing as ngrams, here we use 1-gram or a word \n",
    "                             stop_words=\"english\", # strip out stop words while tokenizing\n",
    "                            )\n",
    "vect = vectorizer.fit_transform(messages) # the blackbox that help us doing everything listed above :P"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total number of vocabulary:  1400\n",
      "\n",
      "Sample of vocabulary\n",
      "=======================\n",
      "\n",
      "abiola\n",
      "able\n",
      "abt\n",
      "ac\n",
      "acc\n",
      "access\n",
      "account\n",
      "activate\n",
      "actually\n",
      "ad\n",
      "add\n",
      "added\n",
      "address\n",
      "admirer\n",
      "advance\n",
      "advice\n",
      "aft\n",
      "afternoon\n",
      "aftr\n",
      "age\n"
     ]
    }
   ],
   "source": [
    "print (\"Total number of vocabulary: \", len(vectorizer.get_feature_names()))\n",
    "print(\"\\nSample of vocabulary\")\n",
    "print(\"=======================\\n\")\n",
    "\n",
    "for i, value in enumerate(vectorizer.get_feature_names()):\n",
    "    if i < 20:\n",
    "        print(value)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total number of messages:  5169\n",
      "Total number of features i.e. words in vocabulary:  1400\n"
     ]
    }
   ],
   "source": [
    "num_samples, num_features = vect.shape\n",
    "print(\"Total number of messages: \", num_samples)\n",
    "print(\"Total number of features i.e. words in vocabulary: \", num_features)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Divide the data into training and testing data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Split the data into a training set and a test set\n",
    "from sklearn.model_selection import train_test_split\n",
    "X_train, X_test, y_train, y_test = train_test_split(vect, labels, test_size=0.3, random_state=0)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Train the model in training data\n",
    "\n",
    "We're going to use the Gaussian Naive Bayes classifier for the implementation. Aah ! You got me. I didn't mention about the variations of Naive Bayes Classifier, did I? :D Don't worry a bit. Naive Bayes is what we've explained in the tutorial but for many other reasons, many smart people have extended and optimized Naive Bayes. [Read more about it here](http://scikit-learn.org/stable/modules/naive_bayes.html). I bet you can easily catch up with the knowledge you've right now. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "ename": "TypeError",
     "evalue": "A sparse matrix was passed, but dense data is required. Use X.toarray() to convert to a dense numpy array.",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-25-0c9474d44dc6>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0msklearn\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mnaive_bayes\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mGaussianNB\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      2\u001b[0m \u001b[0mgnb\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mGaussianNB\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;31m# initialize the Gaussian Naive Bayes.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 3\u001b[0;31m \u001b[0mgnb\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfit\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mX_train\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my_train\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;31m# fit the model in training data\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;32m/Users/jenkins/anaconda/lib/python3.6/site-packages/sklearn/naive_bayes.py\u001b[0m in \u001b[0;36mfit\u001b[0;34m(self, X, y, sample_weight)\u001b[0m\n\u001b[1;32m    180\u001b[0m             \u001b[0mReturns\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    181\u001b[0m         \"\"\"\n\u001b[0;32m--> 182\u001b[0;31m         \u001b[0mX\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mcheck_X_y\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mX\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    183\u001b[0m         return self._partial_fit(X, y, np.unique(y), _refit=True,\n\u001b[1;32m    184\u001b[0m                                  sample_weight=sample_weight)\n",
      "\u001b[0;32m/Users/jenkins/anaconda/lib/python3.6/site-packages/sklearn/utils/validation.py\u001b[0m in \u001b[0;36mcheck_X_y\u001b[0;34m(X, y, accept_sparse, dtype, order, copy, force_all_finite, ensure_2d, allow_nd, multi_output, ensure_min_samples, ensure_min_features, y_numeric, warn_on_dtype, estimator)\u001b[0m\n\u001b[1;32m    519\u001b[0m     X = check_array(X, accept_sparse, dtype, order, copy, force_all_finite,\n\u001b[1;32m    520\u001b[0m                     \u001b[0mensure_2d\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mallow_nd\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mensure_min_samples\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 521\u001b[0;31m                     ensure_min_features, warn_on_dtype, estimator)\n\u001b[0m\u001b[1;32m    522\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mmulti_output\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    523\u001b[0m         y = check_array(y, 'csr', force_all_finite=True, ensure_2d=False,\n",
      "\u001b[0;32m/Users/jenkins/anaconda/lib/python3.6/site-packages/sklearn/utils/validation.py\u001b[0m in \u001b[0;36mcheck_array\u001b[0;34m(array, accept_sparse, dtype, order, copy, force_all_finite, ensure_2d, allow_nd, ensure_min_samples, ensure_min_features, warn_on_dtype, estimator)\u001b[0m\n\u001b[1;32m    378\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0msp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0missparse\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0marray\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    379\u001b[0m         array = _ensure_sparse_format(array, accept_sparse, dtype, copy,\n\u001b[0;32m--> 380\u001b[0;31m                                       force_all_finite)\n\u001b[0m\u001b[1;32m    381\u001b[0m     \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    382\u001b[0m         \u001b[0marray\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0marray\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0marray\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdtype\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mdtype\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0morder\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0morder\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcopy\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mcopy\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/Users/jenkins/anaconda/lib/python3.6/site-packages/sklearn/utils/validation.py\u001b[0m in \u001b[0;36m_ensure_sparse_format\u001b[0;34m(spmatrix, accept_sparse, dtype, copy, force_all_finite)\u001b[0m\n\u001b[1;32m    241\u001b[0m     \"\"\"\n\u001b[1;32m    242\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0maccept_sparse\u001b[0m \u001b[0;32min\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;32mNone\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;32mFalse\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 243\u001b[0;31m         raise TypeError('A sparse matrix was passed, but dense '\n\u001b[0m\u001b[1;32m    244\u001b[0m                         \u001b[0;34m'data is required. Use X.toarray() to '\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    245\u001b[0m                         'convert to a dense numpy array.')\n",
      "\u001b[0;31mTypeError\u001b[0m: A sparse matrix was passed, but dense data is required. Use X.toarray() to convert to a dense numpy array."
     ]
    }
   ],
   "source": [
    "from sklearn.naive_bayes import GaussianNB\n",
    "gnb = GaussianNB() # initialize the Gaussian Naive Bayes.\n",
    "gnb.fit(X_train, y_train) # fit the model in training data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Test the model in test data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "y_pred_gnd = gnb.predict(X_test) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from sklearn.metrics import confusion_matrix\n",
    "cnf_matrix_gnb = confusion_matrix(y_test, y_pred_gnb)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "print(cnf_matrix_gnb)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "y_pred_mnb = mnb.fit(X_train, y_train).predict(X_test)\n",
    "cnf_matrix_mnb = confusion_matrix(y_test, y_pred_mnb)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "print(cnf_matrix_mnb)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
