{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Naive Bayes Classifier \n",
    "\n",
    "**`Do find time to read the history behind Bayes Theorem in the following link before you begin.`**\n",
    "\n",
    "[History of Bayes Theorem](http://lesswrong.com/lw/774/a_history_of_bayes_theorem/)\n",
    "\n",
    "Let's begin with a some kunjappy questions.\n",
    "\n",
    "(`kunjappy` means tiny in Malayalam.)\n",
    "\n",
    "**`1. What is the probability of getting a king from a deck of cards ?`**\n",
    "\n",
    "There is 52 cards in a deck. 4 of them are Kings.\n",
    "\n",
    "♣ A Clubs King\n",
    "\n",
    "♦ A Diamonds King\n",
    "\n",
    "♥ A Hearts King\n",
    "\n",
    "♠ A Spades King\n",
    "\n",
    "So its 4/52.\n",
    "\n",
    "**`2. What is the probablity of getting a face card ?`**\n",
    "\n",
    "There are 52 cards in the deck. 12 of them are face cards.\n",
    "\n",
    "Then P (`getting a face card`) = 12/52\n",
    "\n",
    "**`3. What is the probablity that the picked card is a face card given the picked card is a king ?`**\n",
    "\n",
    "This time question is bit complicated. Let's break this down. \n",
    "\n",
    "It has two parts. \n",
    "\n",
    "_We know that_ the picked card is a king. It is given. \n",
    "\n",
    "_We know that_ all kings are face cards.\n",
    "\n",
    "Then the picked card _must be_ a face card, right ? \n",
    "\n",
    "A _must be possiblity_ is always represented as 1 in probablity. \n",
    "\n",
    "Answer is 1.\n",
    "\n",
    "**`4. What is the probablity that the picked card is a king given the picked card is a face card ?`**\n",
    "\n",
    "Now things are more complicated. But we'll break down this question too. \n",
    "\n",
    "_We know that_ the picked card is a face card. It is given.\n",
    "\n",
    "_We know that_ there are total 12 face cards in a deck of cards.\n",
    "\n",
    "_We know that_ there are 4 kings in those 12 face cards.\n",
    "\n",
    "Things are lucid. Probablity is 4/12.\n",
    "\n",
    "**`Now, I would like to show you the subtlety in these questions.`**\n",
    "\n",
    "First two questions are examples unconditional probablity. \n",
    "\n",
    "Unconditional Probablity, $$P(A) = \\frac{No.\\ of\\ times\\ event\\ A\\ occurs}{Total\\ no.\\ of\\ occurances \\ of\\ events}$$\n",
    "\n",
    "Answer one and two are obvious now. \n",
    "\n",
    "### `What was different in last two questions ?`\n",
    "\n",
    "It's given. I mean literally **_given_**. :D\n",
    "\n",
    "Yup. You guessed it right. It's called **`Conditional Probablity`**.\n",
    "\n",
    "The conditional probability of an **`event B`** is the probability that the event will occur **_given_** the knowledge that an **`event A`** has already occurred. \n",
    "\n",
    "This probability is written **`P(B|A)`**, notation for the **`probability of B given A`**. \n",
    "\n",
    "Conditional Probability, $$P(B\\ |\\ A) = \\frac{P(A\\ and \\ B)}{P(A)}$$\n",
    "\n",
    "given `P(A)` is not zero and events `A` and `B` are dependant.\n",
    "\n",
    "`High school teachers have already taught us this equation, mate. Do better, will you ? `\n",
    "\n",
    "`Ha-Ha ! You bet.`\n",
    "\n",
    "How these terms is related? I'm going to tell you a story, and it's one way of explaining the intuition behind it. [You may find another way in this link.](http://wtmaths.com/conditional_probability_venn_diagrams.html)  \n",
    "\n",
    "-------------------\n",
    "\n",
    "## Marbles of Las Vegas\n",
    "\n",
    "Imagine you went on a trip to Las Vegas. You've got into a casino to try your luck. There you saw an unusual game.\n",
    "\n",
    "Game is simple. There is an opaque bag and it contains 5 identical marbles in it. Their colors are different by the way, 3 green marbles and 2 red marbles. Game is won when you pick 2 green marbles from the bag one after another. There is no replacement of marbles. \n",
    "\n",
    "So, \n",
    "\n",
    "- Your first pick should be a green marble.  \n",
    "- Your second pick should also be a green marble.  \n",
    "- You win the game.\n",
    "\n",
    "As there is an admission charge for playing, you would like to calculate the chances of winning the game in terms of probablitiy.\n",
    "\n",
    "Now, what exactly we're going to calculate here ? It is **`P(getting a green in first pick and second pick`)**.\n",
    "\n",
    "What would it be ? Let's break this up as we did before. \n",
    "\n",
    "`P(Getting a Green marble from the bag)`=3/5\n",
    "\n",
    "`P(Getting a Red marble from the bag)`=2/5\n",
    "\n",
    "Then,\n",
    "\n",
    "`P(green in first pick and green is second pick`) = `P(getting green marble)` x `P(getting a green marble)`\n",
    "\n",
    "$$= \\frac{3}{5} * \\frac{3}{5} = \\frac{9}{25}$$\n",
    "\n",
    "Seems okay ? \n",
    "\n",
    "No. It's not okay. \n",
    "\n",
    "### `Why ?`\n",
    "\n",
    "\n",
    "Imagine you're picking a Green marble in the first try. Now what is left in that bag ? Remember there is no replacement. \n",
    "\n",
    "**`Then the bag will be left with just 2 Green Marbles and 2 Red marbles !!!`**\n",
    "\n",
    "i.e. Next time when you try to pick a Green \n",
    "\n",
    "$$ P(getting\\ a\\ green\\ marble\\ from\\ the\\ bag\\ in\\ second\\ pick) =\\frac{1}{2}$$\n",
    "\n",
    "Or we can put it this way. \n",
    "\n",
    "$$ P(getting\\ a\\ green\\ marble\\ from\\ the\\ bag\\ given\\ first\\ pick\\ is\\ a\\ green)=\\frac{1}{2}$$\n",
    "\n",
    "Aha ! `GIVEN AGAIN!`\n",
    "\n",
    "What do we mean by `given` here ? \n",
    "\n",
    "A situation of number of green marbles reducing to 2, is arised when a green marble is picked at first, right ?. \n",
    "\n",
    "So basically, when we're trying to pick the green marble for the second time, we're calculating the probablity of an event in reference to the first pick of green marbel. **` i.e. calculated probablity of current event is in reference to another event that already happened`**.\n",
    "\n",
    "SO IN THAT CONTEXT, WE HAVE A GIVEN PROBABLITY !  \n",
    "\n",
    "To make the notations easier, let's name the events.\n",
    "\n",
    "`A = Event of getting a Green marble in first pick`\n",
    "\n",
    "`B = Event of getting a Green marble in second pick`\n",
    "\n",
    "Then, $$ P(A\\ and\\ B)= P(A) * P(B\\ given\\ A) = \\frac{3}{5} * \\frac{1}{2} = \\frac{3}{10}$$\n",
    "\n",
    "\n",
    "Mathematically, $$ P(A \\cap B) =  P(A) * P(B\\ |\\ A)$$  \n",
    "Or, $$ P(B\\ |\\ A) = \\frac {P(A \\cap B)}{P(A)} $$  \n",
    "\n",
    "In other words, **`if we know that A has occurred, then the sample space is reduced to the probability of event A, and the denominator for P(A and B) is not 1 but P(A)`**. This is how terms in the equations are related. \n",
    "\n",
    "I hope, now the mystery of conditional probablity equation is unveiled. This is the intuitive part high school teachers didn't tell us:D\n",
    "\n",
    "Back to the game, we have only 30% chance of winning, so it is not worth spending money on it. \n",
    "\n",
    "But we are going to learn more from this game. Here, I would like to make an important statement. \n",
    "\n",
    "###  `A and B are dependent events.`\n",
    "\n",
    "Let's consider the other way around. What would be the scenerio, if there are replacements for marbles everytime there is a pick ? Then, no matter how many times you pick, the probablity will be the same.\n",
    "\n",
    "Back to events A and B, \n",
    "\n",
    "`P(getting a green marble from the bag in first pick), P(A)` = 3/5\n",
    "\n",
    "`P(getting a green marble from the bag in second pick), P(B)`= 3/5\n",
    "\n",
    "i.e, $$ P(A \\cap B) =  P(A) * P(B\\ |\\ A)$$\n",
    "\n",
    "$$= P(A) * P(B)$$\n",
    "\n",
    "$$= \\frac{3}{5} * \\frac{3}{5} = \\frac{9}{25}$$ \n",
    "\n",
    "###  `Thus events A and B are independent`\n",
    "\n",
    "In the case where **`events A and B are independent`** (where `event B` has no effect on the probability of `event A`), the `conditional probability of event B given event A` becomes `P(B)` as the joint probablity is product of `probability of event A` and `probability of event B`, that is ` P(A) X P(B)`.\n",
    "\n",
    "Now you know many things about conditional probablity, think if `P(A|B)` and `P(B|A)` are the same? What about $P(A\\ \\cap \\ B)$ and $P(B\\ \\cap \\ A)$ ? Think and you'll get new intrepretations.\n",
    "\n",
    "-------------------------------\n",
    "\n",
    "## Bayes Theorem\n",
    "\n",
    "I've put the effort in this basic section because, **`Bayes Theorem`** is derived from **`conditional probablity equation`**.\n",
    "\n",
    "We just proved that,  \n",
    "$$ P(B\\ |\\ A) = \\frac {P(A \\cap B)}{P(A)} $$ \n",
    "\n",
    "Similarly, \n",
    "\n",
    "$$ P(A\\ |\\ B) = \\frac {P(A \\cap B)}{P(B)} $$  \n",
    "\n",
    "Then,  \n",
    "$$P(A \\cap B) = P(A\\ |\\ B) * P(B)$$  \n",
    "\n",
    "$$ P(B\\ |\\ A) =  \\frac{P(A\\ |\\ B) * P(B)}{P(A)}$$  \n",
    "This is what we call the **Bayes Theorem**.  \n",
    "\n",
    "Here we're going to make some observations. Before that, let's bring some jargon to our table.\n",
    "\n",
    "The terminology in the Bayesian method of probability (more commonly used) is as follows:\n",
    "\n",
    "- B is called the **`proposition / hypothesis`** and A is called the **`evidence`**.\n",
    "- P(B) is called the **`prior probability of proposition / hypothesis`**\n",
    "- P(A) is called the **`prior probability of evidence`**.\n",
    "- P(B|A) is called the **`posterior`**. \n",
    "- P(A|B) is the **`likelihood`**.\n",
    "\n",
    "So, we may reframe the equation as\n",
    "\n",
    "$$Posterior\\ = Likelihood * \\frac{Prior\\ probablity\\ of\\ Proposition\\ /\\ Hypothesis}{Prior\\ probablity\\ of\\ the\\ evidence}$$\n",
    "\n",
    "Or, \n",
    "\n",
    "$$Posterior = New\\ Information * Initial\\ Knowledge$$\n",
    "\n",
    "You might be tad confused about the intuitive understanding of this equation. Don't worry, I'm going to tell you another small story to clarify it.  \n",
    "\n",
    "## Flu diary\n",
    "\n",
    "During the month of June, a friend of mine who is a doctor, told me to watch out, because Flu was spreading. She warned me about carrying an umbrella to not to get drenched by rain and shared the following table since she knew that I'm a `data person`. \n",
    "\n",
    "| Symptoms    \t| Disease  \t|\n",
    "|:---------:\t|:--------:\t|\n",
    "| Sneezing    \t| Flu      \t|\n",
    "| Sneezing    \t| Cold     \t|\n",
    "| Headache    \t| Migraine \t|\n",
    "| Sneezing    \t| Cold      |\n",
    "| Coughing    \t| Flu       |\n",
    "| Backache    \t| Flu     \t|\n",
    "| Vomiting    \t| Flu \t    |\n",
    "| Vomiting      | Migraine \t|\n",
    "| Temperature \t| Flu      \t|\n",
    "| Drowsiness  \t| Cold     \t|\n",
    "\n",
    "This is a table depicts of 10 patients who had the listed symptoms. Their final diagonisis is appended in the next column. Being a careless chap, I got drenched by rain on very next day while coming back from my office.\n",
    "\n",
    "#### Day One\n",
    "\n",
    "By the time I reached my apartment, I was sneezing. I didn't call her for help (Obvious reasons :D). Being the `data person` I tried to find my diagnosis using data. \n",
    "\n",
    "So I wanted to find out if I've flu given I'm sneezing. \n",
    "\n",
    "Here I'm going to define all the jargon we introduced above.\n",
    "\n",
    "- Hypothesis = Having Flu\n",
    "- Evidence = Sneezing  \n",
    "- Prior probablity of Hypothesis = 5 / 10 ( Out of total 10 patients 5 are affected by Flu)  \n",
    "- Prior probablity of Evidence = 3 / 10 (Out of 10 patients 3 are affected by Sneeze)  \n",
    "- Keep in mind that out of 10 patients 1 had Sneezing as symptom and diagnosis was Flu\n",
    "- Likelihood = \n",
    "\n",
    "\\begin{align}\n",
    "P(Sneezing\\ given\\ Flu)&= \\frac{P(Sneezing\\ and\\ Flu)}{P(Flu)}\\\\ &= \\frac{\\frac{1}{10}}{\\frac{5}{10}}  = \\frac{1}{5}\\end{align}\n",
    "\n",
    "- Posterior =    \n",
    "$$P(Flu\\ given\\ Sneezing) =\\frac {\\frac{5}{10} * \\frac{1}{5}}{\\frac{3}{10}} = \\frac{1}{3} $$\n",
    "\n",
    "So there is 33.33 % chance that I've Flu given I've Sneezing. Not bad. What about Cold ? \n",
    "\n",
    "- Likelihood = \n",
    "\n",
    "$$P(Sneezing\\ given\\ Cold)= \\frac{P(Sneezing\\ and\\ Cold)}{P(Cold)} $$\n",
    "\n",
    "$$= \\frac{\\frac{2}{10}}{\\frac{3}{10}} = \\frac{2}{3}$$\n",
    "\n",
    "- Posterior =  \n",
    "\n",
    "$$P(Cold\\ given\\ Sneezing) =\\frac {\\frac{3}{10} * \\frac{2}{3}}{\\frac{3}{10}} = \\frac{2}{3} $$\n",
    "\n",
    "Wow! I've a greater chance of `cold` than `flu`.\n",
    "\n",
    "What about Migraine? Do I've a chance of Migraine? If so how much it would be? If not what might be the reason? Ask yourself, find your answers and intrepretations.\n",
    "\n",
    "We're are going to make some observations here using the knowledge we've right now. \n",
    "\n",
    "##### Intuition of Prior Probability of hypothesis\n",
    "\n",
    "    - This is the strength in belief / common sense / initial knowledge to quantify an uncertainity without considering any evidences / new information.\n",
    "    - Here consider the \"Disease\" column. The prior probability distribution will be the following and it sums to one. \n",
    "\n",
    "| x    | Flu  | Cold | Migraine |   |\n",
    "|------|------|------|----------|---|\n",
    "| P(x) | 5/10 | 3/10 | 2/10     | 1 | \n",
    "\n",
    "    - Whatever maybe the symptoms i.e. evidence, without considering it, it can be wildly guessed that I've flu. This guess can be wrong once evidence is introduced.             \n",
    "        \n",
    "    - As evidences are added one by one (mathematically speaking it's multiplication) our knowledge will improve and we can make more accurate guesses. \n",
    "    \n",
    "\n",
    "##### Intuition of Likelihood\n",
    "\n",
    "`Posterior` and `Likelihood` seems to be similar from equation i.e. $P(A\\ |\\ B)$ and $P(B\\ |\\ A)$. But are they any how related ? How do we quantify it ? \n",
    "\n",
    "As we see here, `P(Flu given Sneezing)` is entirely different from `P(Sneezing given Flu)`. Likelihood introduces the effect of evidence to the hypothesis, helps to make a better predicition. This is directly drawn from the data and tells us how strong an evidence is, to support our claimed hypothesis. \n",
    "\n",
    "For example, here I claim that I've flu. My evidence for that claim is sneezing. But from the above calculations we've found that, the evidence of sneezing is supporting the claim of cold rather than flu. \n",
    "\n",
    "##### Intuition of Prior Probability of Evidence\n",
    "\n",
    "This shares the same intuition of Prior Probability explained above, but for evidence. By dividing with it, we're implicitly making it sure that, sample space is conditioned to evidence.\n",
    "\n",
    "#### Day Two\n",
    "\n",
    "Bayes theorem can solve many problems but flu is not one of them. I realized it on next day. I started vomiting too!!.  Now I've two symptoms. Cool. Usually people seek medical help at this point, but since in sake of writing this tutorial, I started calculating the posterior probability of my diagnosis :D\n",
    "\n",
    "Here I'm going a really important point. Why Naive-Bayes is naive? What does it mean? \n",
    "\n",
    "I've talked about independant events before and individual probabilities just get multiplied in that case. People call it the **chain rule**. \n",
    "\n",
    "The Naive Bayes algorithm is called \"naive\" because it makes the assumption that the occurrence of a certain feature is independent of the occurrence of other features.  \n",
    "\n",
    "That means, my sneezing and vomiting are not related. Even if these features depend on each other or on the presence of the other features, all of these properties individually contribute to the probability that this is clear case of a flu or cold or migraine and that is why it is known as \"naive.\" \n",
    "\n",
    "How we're going to frame these philosophies mathematically ?  \n",
    "\n",
    "$$P(Flu\\ given\\ vomiting\\ and\\ sneezing) = \\frac{P(Vomiting\\ and\\ Sneezing\\ given\\ Flu) * P(Flu)}{P(Vomiting\\ and\\ Sneezing)}$$\n",
    "\n",
    "But, \n",
    "\n",
    "\\begin{align}\n",
    "P(Vomiting\\ and\\ Sneezing\\ given\\ Flu) * P(Flu) = P(Vomiting\\ and\\ Sneezing\\ and\\ Flu)\n",
    "\\end{align}\n",
    "\n",
    "But RHS can be further expanded as follows,  \n",
    "\n",
    "\\begin{align}\n",
    "P(Vomiting\\ and\\ Sneezing\\ and\\ Flu) & = P(Vomiting\\ |\\ Sneezing\\ and\\ Flu) * P(Sneezing\\ and\\ Flu) \\\\\n",
    "& = P(Vomiting\\ |\\ Sneezing\\ and\\ Flu) * P(Sneezing\\ |\\ Flu) * P(Flu)\n",
    "\\end{align}\n",
    "\n",
    "Because of the assumption that features are independent, \n",
    "\n",
    "$$ P(Vomiting\\ |\\ Sneezing\\ and\\ Flu) = P(Vomiting\\ |\\ Flu)$$\n",
    "\n",
    "Now, \n",
    "\n",
    "$$P(Flu\\ given\\ vomiting\\ and\\ sneezing) = \\frac{P(Vomiting\\ |\\ Flu) * P(Sneezing\\ |\\ Flu) * P(Flu)}{P(Vomiting\\ and\\ Sneezing)}$$\n",
    "\n",
    "\n",
    "Consider Cold,\n",
    "\n",
    "$$P(Cold\\ given\\ vomiting\\ and\\ sneezing) = \\frac{P(Vomiting\\ |\\ Cold) * P(Sneezing\\ |\\ Cold) * P(Cold)}{P(Vomiting\\ and\\ Sneezing)}$$\n",
    "\n",
    "What about Migraine ? \n",
    "\n",
    "$$P(Migraine\\ given\\ vomiting\\ and\\ sneezing) = \\frac{P(Vomiting\\ |\\ Migraine) * P(Sneezing\\ |\\ Migraine) * P(Migraine)}{P(Vomiting\\ and\\ Sneezing)}$$\n",
    "\n",
    "Now note that $P(Vomiting\\ and\\ Sneezing)$ is not really making a difference in any of the calculations, let's treat it as a constant.\n",
    "\n",
    "Then the equations will be, \n",
    "\n",
    "$$P(Flu\\ given\\ vomiting\\ and\\ sneezing) \\propto P(Vomiting\\ |\\ Flu) * P(Sneezing\\ |\\ Flu) * P(Flu)$$  \n",
    "$$P(Cold\\ given\\ vomiting\\ and\\ sneezing) \\propto P(Vomiting\\ |\\ Cold) * P(Sneezing\\ |\\ Cold) * P(Cold)$$  \n",
    "$$P(Migraine\\ given\\ vomiting\\ and\\ sneezing) \\propto P(Vomiting\\ |\\ Migraine) * P(Sneezing\\ |\\ Migraine) * P(Migraine)$$   \n",
    "\n",
    "\n",
    "\n",
    "Let's calculate these probabilities one by one starting with Flu. \n",
    "\n",
    "We already know the value of $ P(Sneezing\\ |\\ Flu)$. Let's get the value of $P(Vomiting\\ |\\ Flu)$.  \n",
    "\n",
    "$$P(Vomiting\\ |\\ Flu) = \\frac{P(Vomiting\\ and\\ Flu)}{P(Flu)}$$\n",
    "\n",
    "\\begin{align}\n",
    "P(Vomiting\\ given\\ Flu) &= \\frac{\\frac{1}{10}}{\\frac{5}{10}} = \\frac{1}{5}\n",
    "\\end{align}\n",
    "\n",
    "Finally, \n",
    "\n",
    "$$P(Flu\\ given\\ vomiting\\ and\\ sneezing) \\propto \\frac{1}{5} * \\frac{1}{5} *\\frac{5}{10} = \\frac{1}{50}$$\n",
    "\n",
    "What about cold, like last time? \n",
    "\n",
    "\\begin{align}\n",
    "P(Cold\\ given\\ vomiting\\ and\\ sneezing)& \\propto P(Vomiting\\ given\\ Cold) * P(Sneezing\\ given\\ Cold) * P(Cold)\\\\&=0* \\frac{2}{5} *\\frac{3}{10} = 0\\end{align}\n",
    "Oops! ZERO. Migraine? \n",
    "\n",
    "\\begin{align}\n",
    "P(Migraine\\ given\\ vomiting\\ and\\ sneezing)& \\propto P(Vomiting\\ given\\ Migraine) * P(Sneezing\\ given\\ Migrane) * P(Migraine)\\\\&= \\frac{1}{2} *0 *\\frac{2}{10} = 0\\end{align}\n",
    "\n",
    "HUH! Confirmed. She was right. Flu was actually... you know. Better I get to hospital than tinkering around with posteriors :P \n",
    "\n",
    "Here we've calculated three probabilities to infer the most probable disease I've. Mathematically, this process called [Maximum Likelihood Estimation](https://en.wikipedia.org/wiki/Maximum_likelihood_estimation). Once you load Naive-Bayes classifier with a training data, it calculates this estimate and for a new test case, inference is made based on this estimation. \n",
    "\n",
    "If you notice, we needed two parameters to calculate this estimate and it was calcated from the counts of those cases in training data. \n",
    "\n",
    "- P(Label) \n",
    "    - P(Flu) = Count of `Flu` records / Total records\n",
    "- Product of Probabilities of features conditioned on the label \n",
    "    - P(Vomiting given Flu), P(Sneezing given Flu) \n",
    "    - Again from counts of those cases in training data. \n",
    "    \n",
    "    \n",
    "Woo-Hoo! We've just implemented our **NAIVE BAYES CLASSIFIER**. Yes, it's that simple. I haven't used any mathematical notations in the entire tutorial because once a wise man said that each math formula in your work will halfen the count of your readers. :D\n",
    "\n",
    "Jokes apart, I strongly recommend my readers to  visit [here](https://www.probabilitycourse.com/chapter1/1_4_0_conditional_probability.php), [here](http://www.cs.columbia.edu/~mcollins/em.pdf) and [here](https://oscarbonilla.com/2009/05/visualizing-bayes-theorem/) to understand the mathematical framing behind it.\n",
    "\n",
    "There's a bunch of inabilities for our \"yet hero\" NB Classifier. Read about it and intepret them. I'll describe one here. \n",
    "\n",
    "We've made an impactful assumption here while modelling our classifier. The features are independent to each other. Though it helped us simplifying our derivation, it's quite impossible to find such kind of a problem in real life. Think about it and come up with your own philosophies. Well, it's all about philosophies, ain't it? \n",
    "\n",
    "Shall we start coding ? ;)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
